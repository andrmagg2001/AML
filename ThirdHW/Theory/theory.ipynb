{"cells":[{"cell_type":"markdown","metadata":{"id":"FZZRKQXzLqbT"},"source":["# AML Homework 3: Theory\n"]},{"cell_type":"markdown","metadata":{"id":"oGlqnTYqA_lp"},"source":["## Table of contents:\n","* [The Transformer Architecture](#transformer) - **11 Points**\n","    * Understanding the Attention Mechanism\n","    * Scaled Dot Product Attention - **1 Point**\n","    * Multi-Head Attention - **2 Points**\n","    * The Encoder - Decoder Block - **3 Points**\n","    * Positional Encoding - **1 Point**\n","    * Transformer Network - **2 Points**\n","    * Optimizer - **1 Point**\n","    * Question - **1 Point**\n","* [Graph Metanetworks](#gmn) **6 Points**\n","    * A glimpse on the neural network to parameter graph conversion\n","    * Loading some parameter graphs\n","    * MPNN implementation\n","        * Edge Model - **1 Point**\n","        * Node Model - **2 Points**\n","        * Global Model - **1 Point**\n","        * MPNN - **2 Points**"]},{"cell_type":"markdown","metadata":{"id":"ld6F6GDJA_ls"},"source":["## Initial Setup\n","Run the following cells to sync with Google Drive if you run from Google Colab, and to install the required torch_scatter library."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3056,"status":"ok","timestamp":1727269528355,"user":{"displayName":"Leonardo Plini","userId":"13271473654794383474"},"user_tz":-120},"id":"WhC7DfPAA_ls","outputId":"222f0d04-5a65-4a8f-94a3-6af02b7d9948"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"CpvvewXALqbX"},"source":["**Replace the path in the cd command with yours**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727269557637,"user":{"displayName":"Leonardo Plini","userId":"13271473654794383474"},"user_tz":-120},"id":"re0oSK4SA_lt","outputId":"a1caacb9-5189-4231-8376-08b80c4f71a9"},"outputs":[],"source":["%cd /content/drive/MyDrive/HW03_AML2425/Theory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VC6yWlg-LqbX"},"outputs":[],"source":["!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.4.0%2Bcu121.html"]},{"cell_type":"markdown","metadata":{"id":"le48bnfJA_lt"},"source":["## The Transformer Architecture <a class=\"anchor\" id=\"transformer\"></a>"]},{"cell_type":"markdown","metadata":{"id":"GdyMOogLA_lu"},"source":["This notebook serves as a comprehensive guide to the fundamental components of the Transformer model, a highly influential architecture in deep learning. Since the release of the seminal paper by Vaswani et al. titled [Attention Is All You Need](https://arxiv.org/abs/1706.03762) in 2017, the Transformer design has consistently surpassed performance benchmarks, particularly in the field of natural language processing. Transformers equipped with a vast number of parameters have demonstrated the ability to generate extensive and compelling text, thus opening up new frontiers in AI applications.\n","It is imperative to gain a thorough understanding of the inner workings of the Transformer architecture and to be able to implement it independently, a task we will accomplish within the context of this notebook."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"y3nLcU1iSAey"},"outputs":[],"source":["import torch\n","from torch.nn.functional import softmax\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","import copy\n","from copy import deepcopy\n","import numpy as np\n","import math\n","import scipy.io\n","import os\n","import random\n","\n","\n","## Imports for plotting\n","import matplotlib.pyplot as plt\n","plt.set_cmap('cividis')\n","%matplotlib inline\n","from IPython.display import set_matplotlib_formats\n","from matplotlib.colors import to_rgb\n","import matplotlib\n","matplotlib.rcParams['lines.linewidth'] = 2.0\n","import seaborn as sns\n","sns.reset_orig()\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","np.random.seed(0)\n","random.seed(0)\n","\n","torch.backends.cudnn.deterministic=True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"id":"nh5tM4toA_lw"},"source":["### Understanding the Attention Mechanism <a class=\"anchor\" id=\"att_mechanism\"></a>\n","\n","In recent years, particularly in sequence-related tasks, the attention mechanism has emerged as a crucial component within neural networks. This mechanism comprises a set of layers that have gained substantial attention due to their effectiveness. The primary purpose of the attention mechanism is to compute a weighted average of elements within a sequence. These weights are dynamically determined based on an input query and the keys associated with the elements. But what does this exactly entail?\n","\n","Essentially, the goal is to calculate an average that takes into account the true values of each element, rather than assigning equal weight to all. To achieve this, an attention mechanism typically consists of four key components:\n","\n","* **Query:** The query is a feature vector that helps identify specific elements within the sequence that require attention or consideration.\n","\n","* **Keys**: Each input element is associated with a key, which is also a feature vector. These keys provide insights into what each element contributes or when it becomes relevant. They are designed to enable the identification of elements that deserve attention based on the query.\n","\n","* **Values**: For each input element, there is a corresponding value vector. The aim is to compute an average using these value vectors.\n","\n","* **Score Function:** To determine the items deserving of attention, a scoring function denoted as $f_{attn}$ must be defined. This function takes the query and a key as inputs and yields both the attention weight and score for the query-key pair. Typically, common similarity metrics such as dot products or simple multi-layer perceptrons (MLPs) are employed for this purpose.\n","\n","***How are key (K), query (Q) and value (V) computed?***\n","In these formulas, we'll denote the original representations as $(x_i)$ for each element in the sequence.\n","\n","1. **Key (K) Computation**:\n","   - The key vector for each element $i$ is computed by multiplying the original representation $x_i$ by a learned key weight matrix $W^K$.\n","   - Mathematically, the key vector $k_i$ is obtained as follows:\n","\n","     $k_i = x_i \\cdot W^K$\n","\n","2. **Query (Q) Computation**:\n","   - Similarly, the query vector for each element $i$ is computed by multiplying the original representation $x_i$ by a learned query weight matrix $W^Q$.\n","   - Mathematically, the query vector $q_i$ is obtained as follows:\n","\n","     $q_i = x_i \\cdot W^Q$\n","\n","3. **Value (V) Computation**:\n","   - The value vector for each element $i$ is computed by multiplying the original representation $x_i$ by a learned value weight matrix $W^V$.\n","   - Mathematically, the value vector $v_i$ is obtained as follows:\n","\n","     $v_i = x_i \\cdot W^V$\n","\n","Here's a bit more explanation:\n","\n","- $x_i$ represents the original representation (e.g., word embedding or feature vector) of the $i$-th element in the sequence.\n","\n","- $W^K$, $W^Q$, and $W^V$ are learnable weight matrices specific to the key, query, and value computations, respectively. These weight matrices are shared across all elements in the sequence but may have different dimensions based on the desired dimensionality of the key, query, and value spaces.\n","\n","- After computing the key, query, and value vectors for each element in the sequence, these vectors are used in the self-attention mechanism to calculate attention scores, which determine how much each element attends to others in the sequence. This process is typically followed by a weighted sum of the value vectors to obtain the final output for each element.\n","\n","To obtain the weights for averaging, a softmax function is applied to the scores produced by the scoring function across all elements. Consequently, value vectors associated with keys most similar to the query receive higher weights in the averaging process.\n","\n","$$\n","\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{K}_i, \\text{Q}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{K}_j, \\text{Q}\\right)\\right)}, \\hspace{5mm} \\text{out}_i = \\sum_i \\alpha_i \\cdot \\text{V}_i\n","$$\n","\n","Here is an example of attention over a sequence:\n","\n","<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/attention_example.svg?raw=1\" width=\"750px\"></center>\n","\n","In this scenario, each word in the sequence has an associated key and value. The scoring function evaluates the similarity between the query and all the keys to determine the weights. These attention weights are then used to compute the weighted average of the word values.\n","\n","It's important to note that self-attention is a variant of attention applied within the Transformer architecture. In self-attention, each element in the sequence serves as both a key and a value and undergoes an attention layer. This layer assesses the similarity between the keys of all sequence elements based on the query of each element, ultimately producing unique averaged value vectors for each element."]},{"cell_type":"markdown","metadata":{"id":"-qaBeNN7A_ly"},"source":["### Scaled Dot Product Attention  (**1 Point**) <a class=\"anchor\" id=\"scaled_dot_product\"></a>\n","\n","The core concept behind self-attention is the scaled dot product attention, which aims to create an efficient attention mechanism that enables each element within a sequence to attend to every other element. This mechanism is designed to strike a balance between computational efficiency and expressive power.\n","\n","The inputs to the dot product attention consist of queries ($Q\\in\\mathbb{R}^{T\\times d_k}$), keys ($K\\in\\mathbb{R}^{T\\times d_k}$), and values ($V\\in\\mathbb{R}^{T\\times d_v}$). Here, $T$ represents the sequence length, while $d_k$ and $d_v$ denote the hidden dimensions for $Q$, $K$, and $V$.\n","\n","The dot product attention is computed as follows:\n","\n","$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n","\n","The matrix multiplication $QK^T$ produces a matrix with dimension $T\\times T$ by doing the dot product for every distinct pair of queries and keys. The attention logits for a particular element $i$ to every other element in the sequence are shown in each row. We use a softmax on these and multiply by the value vector to get a weighted mean (the weights being determined by the attention). The computation graph below provides another viewpoint on this attention technique.\n","\n","<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/scaled_dot_product_attn.svg?raw=1\" width=\"210px\"></center>\n","\n","$1/\\sqrt{d_k}$, the scaling factor, is crucial to maintain an appropriate variance of attention values after initialization. As a result, $Q$ and $K$ may also have a variance of close to $1$.\n","\n","\n","*Note: Keep in mind that we initialize our layers with the purpose of having equal variance across the model. Dot products over two vectors with variances $\\sigma^2$, however, produce scalars with $d_k$-times larger variance:*\n","\n","$$q_i \\sim \\mathcal{N} (0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k$$\n","\n","\n","*The optional masking of particular entries in the attention matrix is shown by the block labeled \"Mask (opt.)\" in the diagram above. When calculating the attention values, we pad the sentences to the same length and mask out the padding tokens.*\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"3Cv2BSG5A_l-"},"source":["After the discussion regarding the scaled dot-product attention mechanism, please proceed to finalize the code for the `Attention` class as illustrated below."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ZgYad7IHKEkY"},"outputs":[],"source":["class Attention(nn.Module):\n","    ''' Scaled Dot-Product Attention '''\n","\n","    def __init__(self, attn_dropout=0.1):\n","        super().__init__()\n","        self.dropout = nn.Dropout(attn_dropout)\n","\n","    def forward(self, query, key, value, mask=None):\n","\n","        '''\n","        add here the code regarding the argument of the softmax function as defined above\n","        '''\n","        QK = query @ key.transpose(-2, -1)\n","\n","        d_k = key.shape[-1]\n","        \n","        attn = QK / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n","\n","        if mask is not None:\n","            attn = attn.masked_fill(mask == 0, -1e9)\n","\n","        attn = self.dropout(F.softmax(attn, dim=-1))\n","\n","        '''\n","        Computed attn, calculate the final output of the attention layer\n","        '''\n","\n","        output = attn @ value\n","\n","        return output, attn\n"]},{"cell_type":"markdown","metadata":{"id":"KjPYD5CNKEkZ"},"source":["**Do not modify the code below.**\n","\n","After implementing the scaled dot-product attention mechanism, let's proceed with the completion of the `Attention` class below. For this initial implementation, we will not include the mask, which will be introduced and utilized in a subsequent step when building the `MultiHeadAttention` class."]},{"cell_type":"markdown","metadata":{"id":"Qkt0l13yA_mA"},"source":["Some random $Q$, $K$ and $V$ are generated to compute some attention outputs."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432,"status":"ok","timestamp":1727279613143,"user":{"displayName":"Leonardo Plini","userId":"13271473654794383474"},"user_tz":-120},"id":"iFkgi864329q","outputId":"44ccc4bc-692f-487d-94db-64025a7be44c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Q\n"," tensor([[ 1.5410, -0.2934],\n","        [-2.1788,  0.5684],\n","        [-1.0845, -1.3986]])\n","K\n"," tensor([[ 0.4033,  0.8380],\n","        [-0.7193, -0.4033],\n","        [-0.5966,  0.1820]])\n","V\n"," tensor([[-0.8567,  1.1006],\n","        [-1.0712,  0.1227],\n","        [-0.5663,  0.3731]])\n","Output\n"," tensor([[-0.9328,  0.8123],\n","        [-0.9093,  0.3966],\n","        [-0.9970,  0.3056]])\n","Attention\n"," tensor([[0.6291, 0.2395, 0.2425],\n","        [0.1387, 0.4749, 0.4975],\n","        [0.0842, 0.6800, 0.3469]])\n"]}],"source":["torch.manual_seed(0)\n","\n","seq_len, d_k = 3, 2\n","q = torch.randn(seq_len, d_k)\n","k = torch.randn(seq_len, d_k)\n","v = torch.randn(seq_len, d_k)\n","attention = Attention()\n","output, attn = attention(q, k, v)\n","print(\"Q\\n\", q)\n","print(\"K\\n\", k)\n","print(\"V\\n\", v)\n","print(\"Output\\n\", output)\n","print(\"Attention\\n\", attn)\n"]},{"cell_type":"markdown","metadata":{"id":"hWlZbDXSA_mB"},"source":["### Multi-Head Attention  (**2 Points**) <a class=\"anchor\" id=\"multi_head\"></a>\n","\n","\n","A network can effectively focus on various aspects of a sequence, thanks to the scaled dot product attention mechanism. However, for sequence elements, a single weighted average often falls short because they may need to consider multiple distinct characteristics. To address this limitation, we enhance the attention mechanism by introducing multiple heads, each equipped with its own set of query-key-value triplets applied to the same input features. Essentially, we transform a single query, key, and value matrix into $h$ sub-queries, sub-keys, and sub-values, and then independently process them through the scaled dot product attention. These individual head outputs are subsequently combined using a final weight matrix through concatenation.\n","\n","$$\n","\\begin{split}\n","    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n","    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n","\\end{split}\n","$$\n","\n","We refer to this as Multi-Head Attention layer. We can visually see it here:\n","\n","<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/multihead_attention.svg?raw=1\" width=\"230px\"></center>\n","\n","Set the feature map, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$ (with $B$ as the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$). The weights $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding queries, keys, and values of the input. The final result is produced by multiplying the concatenated output by the weight matrix $W^{0}$\n"]},{"cell_type":"markdown","metadata":{"id":"AcxaSTkBA_mC"},"source":["Complete the `MultiHeadAttention` class below."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"b8xqyzQRA_mC"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, num_heads, d_model, dropout=0.1):\n","        \"\"\"\n","        Take in model size and number of heads.\n","        \"\"\"\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0\n","        #  We assume d_v always equals d_k\n","        self.d_k = d_model // num_heads\n","        self.num_heads = num_heads\n","        self.query_ff = nn.Linear(d_model, d_model)\n","        self.key_ff = nn.Linear(d_model, d_model)\n","        self.value_ff = nn.Linear(d_model, d_model)\n","        self.attn_ff = nn.Linear(d_model, d_model)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.attention = Attention(attn_dropout=dropout)\n","\n","    def forward(self, query, key, value, mask=None, return_attention=False):\n","\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","\n","        # 1) Do all the linear projections in batch from d_model => h x d_k.\n","        # The query is given as example, you should do the same for key and value\n","        query = self.query_ff(query).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n","        '''\n","        Add your code below\n","        '''\n","        key = self.key_ff(key).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n","        value = self.value_ff(value).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n","\n","        # 2) Apply attention on all the projected vectors in batch.\n","        '''\n","        Add your code below\n","        '''\n","        x, self.attn = self.attention(query, key, value, mask)\n","        \n","\n","        # 3) \"Concat\" using a view and apply a final linear.\n","        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.num_heads * self.d_k)\n","\n","        if return_attention:\n","            return self.attn_ff(x), self.attn\n","\n","        return self.attn_ff(x)\n"]},{"cell_type":"markdown","metadata":{"id":"T-7PDPT5A_mC"},"source":["**Do not change the following code.**"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"RunMTxMlA_mC","outputId":"cd3c42cf-7d02-4250-e914-1174f75c7c0b"},"outputs":[{"data":{"text/plain":["tensor([[[ 0.2365, -0.1411,  0.0168,  ..., -0.2253,  0.1767,  0.2225],\n","         [ 0.1931, -0.0895,  0.0500,  ..., -0.2019,  0.1098,  0.2202],\n","         [ 0.2050, -0.1029, -0.0381,  ..., -0.1815,  0.1637,  0.2313],\n","         ...,\n","         [ 0.1796, -0.1051,  0.0153,  ..., -0.1890,  0.0949,  0.2301],\n","         [ 0.2132, -0.1249,  0.0418,  ..., -0.2321,  0.1163,  0.2321],\n","         [ 0.1697, -0.0129,  0.0232,  ..., -0.1104,  0.1187,  0.1810]]],\n","       grad_fn=<ViewBackward0>)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(0)\n","np.random.seed(0)\n","\n","num_heads = 8\n","d_model = 512\n","\n","\n","self_attn = MultiHeadAttention(num_heads, d_model)\n","\n","\n","x = torch.tensor(np.random.rand(1, 7,512)).float()\n","\n","attn_out = self_attn(x, x, x)\n","attn_out"]},{"cell_type":"markdown","metadata":{"id":"kyqcnWVpA_mD"},"source":["An essential feature of the multi-head attention mechanism is its **permutation-equivariance** concerning input elements, a critical aspect of this framework. In practical terms, if we were to interchange the first and second items within the input sequence, the output remains entirely unchanged. This property signifies that multi-head attention views the input not as a strict sequence but rather as a collection of items. It is this very characteristic that gives the Transformer architecture and the multi-head attention block their remarkable potency and versatility."]},{"cell_type":"markdown","metadata":{"id":"e9-c-5fHA_mD"},"source":["### The Encoder-Decoder Block (**3 Points**) <a class=\"anchor\" id=\"encoder_decoder\"></a>\n","\n","The original Transformer model, as presented in the paper, was designed primarily for neural machine translation tasks, where it excels at translating sentences from one language to another, such as English to French. The key architectural concept used in the Transformer is the encoder-decoder architecture. In this setup, the encoder processes an input sentence, extracting meaningful features, which are then leveraged by the decoder to generate an output sentence, effectively performing translation.\n","\n","The completet Transformer architecture is illustrated below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).:\n","\n","<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/transformer_architecture.svg?raw=1\" width=\"400px\"></center>\n","\n","Let's examine the Encoder block more in depth. Understanding it will result in an easier comprehension of the Decoder block.\n","\n","\n","The encoder is constructed by applying a sequence of identical blocks, denoted as $N$. Given an input $x$, the initial operation is the application of a Multi-Head Attention block. Subsequently, the output is augmented with the original input using a residual connection, and the sum is then normalized through a layer normalization. This process is formally represented as:\n","\n"," $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ being $Q$, $K$ and $V$ input to the attention layer).\n","\n","Residual connections are instrumental in ensuring a smooth gradient flow throughout the model and preserving vital information about the original sequence.\n","\n","Layer normalization serves multiple purposes—it accelerates training, provides a degree of regularization, and maintains consistent feature magnitudes across the sequence elements.\n","\n","Additionally, a small fully connected feed-forward network (FFN) is incorporated into the model, applied uniformly to each position. The transformation, inclusive of the residual connection, can be summarized as:\n","\n","$$\n","\\begin{split}\n","    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n","    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n","\\end{split}\n","$$\n","\n","To further enhance model robustness and prevent overfitting, dropout layers are strategically employed in the MLP, both on its output and in conjunction with the Multi-Head Attention as regularization measures."]},{"cell_type":"markdown","metadata":{"id":"R4tE9P8VA_mD"},"source":["Add your solution to the `EncoderBlock` and `DecoderBlock` classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsrr_V_cA_mD"},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","\n","    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n","        \"\"\"\n","        Inputs:\n","            input_dim - Dimensionality of the input\n","            num_heads - Number of heads to use in the attention block\n","            dim_feedforward - Dimensionality of the hidden layer in the MLP\n","            dropout - Dropout probability to use in the dropout layers\n","        \"\"\"\n","        super().__init__()\n","\n","        # Attention layer\n","        self.self_attn = MultiHeadAttention(num_heads, input_dim)\n","\n","        # Two-layer MLP\n","        self.linear_net = nn.Sequential(\n","            nn.Linear(input_dim, dim_feedforward),\n","            nn.Dropout(dropout),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(dim_feedforward, input_dim)\n","        )\n","\n","        # Layers to apply in between the main layers\n","        self.norm1 = nn.LayerNorm(input_dim)\n","        self.norm2 = nn.LayerNorm(input_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        # Self_attention part (use self.norm1)\n","        '''\n","        Add your code below\n","        '''\n","\n","        # MLP part (use self.norm2)\n","        '''\n","        Add your code below\n","        '''\n","\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hk8ozyFjA_mD"},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","\n","    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n","        \"\"\"\n","        Inputs:\n","            input_dim - Dimensionality of the input\n","            num_heads - Number of heads to use in the attention block\n","            dim_feedforward - Dimensionality of the hidden layer in the MLP\n","            dropout - Dropout probability to use in the dropout layers\n","        \"\"\"\n","        super().__init__()\n","\n","        # Self Attention layer\n","        self.self_attn = MultiHeadAttention(num_heads, input_dim)\n","        # Attention Layer\n","        self.src_attn = MultiHeadAttention(num_heads, input_dim)\n","\n","        # Two-layer MLP\n","        self.linear_net = nn.Sequential(\n","            nn.Linear(input_dim, dim_feedforward),\n","            nn.Dropout(dropout),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(dim_feedforward, input_dim)\n","        )\n","\n","        # Layers to apply in between the main layers\n","        self.norm1 = nn.LayerNorm(input_dim)\n","        self.norm2 = nn.LayerNorm(input_dim)\n","        self.norm3 = nn.LayerNorm(input_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        # Self-Attention part (use self.norm1)\n","        '''\n","        Add your code below\n","        '''\n","\n","        # Attention part (use self.norm2)\n","        # Recall that memory is the output of the encoder and replaces x as\n","        # the key and value in the attention layer\n","        '''\n","        Add your code below\n","        '''\n","\n","        # MLP part (use self.norm3)\n","        '''\n","        Add your code below\n","        '''\n","\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"ke1i7zpNA_mE"},"source":["### Positional Encoding  (**1 Point**)\n","\n","Positional information plays a vital role in tasks like language understanding, where the order of words in a sequence is crucial. To incorporate this positional context into our model, we can utilize positional encoding. Even if we were to learn embeddings for every possible position, it would not be feasible for sequences of varying lengths. Therefore, a more practical approach is to employ feature patterns that the network can discern from the input features and potentially generalize to longer sequences.\n","\n","Following the solution of Vaswani et al., the positional encoding is defined as:\n","\n","$$\n","PE_{(pos,i)} = \\begin{cases}\n","    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n","    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n","\\end{cases}\n","$$\n","\n","In this equation, $PE_{(pos, i)}$ represents the positional encoding value at position $pos$ within the sequence and hidden dimension $i$. The combination of these values forms the positional information, which is added to the initial input features and concatenated across all hidden dimensions. This strategy allows the model to capture and utilize positional context effectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wK9ueR5gKEkb"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    Implement the PE function.\n","    \"\"\"\n","\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        '''\n","        Add your code below\n","        '''\n","        pe = pe.unsqueeze(0) # the final dimension is (1, max_len, d_model)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n","        return self.dropout(x)\n"]},{"cell_type":"markdown","metadata":{"id":"KVGi5FZoKEkb"},"source":["To gain a deeper understanding of positional encoding, we can visualize it. We'll generate a sequence-based image that represents positional encoding across hidden dimensions. In this visualization, each pixel will signify the adjustment made to the input feature to encode a specific position.\n","\n","**Do not change the following code.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":319},"id":"ey8R8ei5KEkb","outputId":"abce9fe5-5498-4d3c-e20c-4ce02a950cca"},"outputs":[],"source":["encod_block = PositionalEncoding(d_model=48, dropout=0.1, max_len=96)\n","pe = encod_block.pe.squeeze().T.cpu().numpy()\n","\n","fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n","pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n","fig.colorbar(pos, ax=ax)\n","ax.set_xlabel(\"Position in sequence\")\n","ax.set_ylabel(\"Hidden dimension\")\n","ax.set_title(\"Positional encoding over hidden dimensions\")\n","ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n","ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1-OhCmxYKEkb"},"source":["The sine and cosine waves with various wavelengths that encode the position in the hidden dimensions are easily visible. To better understand the pattern, we can examine the sine/cosine wave for each hidden dimension separately. The positional encoding for the hidden dimensions is shown in the image below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414},"id":"FnLQ-p4-KEkb","outputId":"09471950-506e-41dc-9ea0-fca184e9ced3"},"outputs":[],"source":["sns.set_theme()\n","fig, ax = plt.subplots(2, 2, figsize=(12,4))\n","ax = [a for a_list in ax for a in a_list]\n","for i in range(len(ax)):\n","    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n","    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n","    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n","    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n","    ax[i].set_xticks(np.arange(1,17))\n","    ax[i].tick_params(axis='both', which='major', labelsize=10)\n","    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n","    ax[i].set_ylim(-1.2, 1.2)\n","fig.subplots_adjust(hspace=0.8)\n","sns.reset_orig()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bLI5sBXqA_mF"},"source":["### Transformer Network  (**2 Points**)\n","Everything we've talked about up to this point is summarized in the `Transformer` class. You will need all of the components (`EncoderBlock`, `DecoderBlock` and `PositionalEncoding`) previously seen to complete it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuZ4CCsaA_mF"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, enc_inp_size, dec_inp_size, dec_out_size, N=6,\n","                   d_model=512, dim_feedforward=2048, num_heads=8, dropout=0.1,\n","                   mean=[0,0],std=[0,0]):\n","        super(Transformer, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.dim_feedforward = dim_feedforward\n","        self.dropout = dropout\n","        self.N = N\n","        self.mean = mean\n","        self.std = std\n","        self.enc_inp_size = enc_inp_size\n","        self.dec_inp_size = dec_inp_size\n","        self.dec_out_size = dec_out_size\n","\n","        self.encoder = nn.ModuleList([deepcopy(\n","            EncoderBlock(d_model, num_heads, dim_feedforward, dropout)) for _ in range(N)])\n","        self.decoder = nn.ModuleList([deepcopy(\n","            DecoderBlock(d_model, num_heads, dim_feedforward, dropout)) for _ in range(N)])\n","        self.pos_enc = PositionalEncoding(d_model, dropout)\n","        self.pos_dec = PositionalEncoding(d_model, dropout)\n","        self.src_embed = nn.Linear(enc_inp_size, d_model)\n","        self.tgt_embed = nn.Linear(dec_inp_size, d_model)\n","        self.out = nn.Linear(d_model, dec_out_size)\n","\n","        self.init_weights()\n","\n","\n","    def forward(self, src, trg, src_mask, trg_mask):\n","\n","        # First part of the forward pass: embedding and positional encoding\n","        # both for the source and target\n","        '''\n","        Add your code below\n","        '''\n","\n","        # Second part of the forward pass: the encoder and decoder layers.\n","        # Look at the arguments of the forward pass of the encoder and decoder\n","        # and recall that the encoder output is used as the memory in the decoder.\n","        '''\n","        Add your code below\n","        '''\n","\n","        return output\n","\n","\n","    # Initialize parameters with Glorot / fan_avg.\n","    def init_weights(self):\n","        for p in self.encoder.parameters():\n","            if p.dim() > 1: nn.init.xavier_uniform_(p)\n","        for p in self.decoder.parameters():\n","            if p.dim() > 1: nn.init.xavier_uniform_(p)\n","        for p in self.pos_enc.parameters():\n","            if p.dim() > 1: nn.init.xavier_uniform_(p)\n","        for p in self.pos_dec.parameters():\n","            if p.dim() > 1: nn.init.xavier_uniform_(p)\n","        for p in self.src_embed.parameters():\n","            if p.dim() > 1: nn.init.xavier_uniform_(p)\n","        for p in self.tgt_embed.parameters():\n","            if p.dim() > 1: nn.init.xavier_uniform_(p)\n","        for p in self.out.parameters():\n","            if p.dim() > 1: nn.init.xavier_uniform_(p)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l_eqOB-jLqbc"},"source":["### Optimizer  (**1 Point**)\n","\n","Here we select same **optimizer** proposed into original Transformer paper.\n","\n","It uses some initial warmup epochs, where the learning rate is increased. Then it slowly decreases according to a number of epoch and the chosen embedding size. The resulting formula is:\n","\n","LR = $\\frac{F}{\\sqrt{D}} min( \\frac{1}{\\sqrt{epoch}},\\ epoch \\cdot W^{-\\frac{3}{2}}) $\n","\n","where F is a scaling factor, D is the model embedding size, W is the number of warmup epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltpA13lSLqbc"},"outputs":[],"source":["class SqrtScheduler(torch.optim.lr_scheduler._LRScheduler):\n","\n","    def __init__(self, optimizer, factor, model_size, warmup, max_iters):\n","        self.warmup = warmup\n","        self.max_num_iters = max_iters\n","        self.factor = factor\n","        self.model_size = model_size\n","        super().__init__(optimizer)\n","\n","    def get_lr(self):\n","        lr_factor = self.get_lr_factor(epoch=self.last_epoch+1)\n","        return [base_lr + lr_factor for base_lr in self.base_lrs]\n","\n","    def get_lr_factor(self, epoch):\n","        '''\n","        Add here your code\n","        '''\n","        return lr_factor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gbtq_Ij0Lqbd","outputId":"4341e87f-3e7d-45bc-cc92-5c32ef67099c"},"outputs":[],"source":["# Needed for initializing the lr scheduler\n","p = nn.Parameter(torch.empty(4,4))\n","optimizer = torch.optim.Adam([p], lr=0)\n","\n","lr_scheduler = SqrtScheduler(optimizer=optimizer, factor = 1.0, model_size = 512, warmup=10, max_iters=2000)\n","\n","# Plotting\n","epochs = list(range(2000))\n","sns.set_theme()\n","plt.figure(figsize=(8,3))\n","plt.plot(epochs, [lr_scheduler.get_lr_factor(e+1) for e in epochs])\n","plt.ylabel(\"Learning rate factor\")\n","plt.xlabel(\"Iterations (in batches)\")\n","plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n","plt.show()\n","sns.reset_orig()"]},{"cell_type":"markdown","metadata":{"id":"LKmiKTH8A_mG"},"source":["**Do not change the following code. It is used as a sanity check to verify the good implementation of your code.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-kUgKm75A_mG","outputId":"904f3f6c-4ca6-4af9-b3bd-c540caac570d"},"outputs":[],"source":["# Select GPU device for the training if available\n","if not torch.cuda.is_available():\n","    device=torch.device(\"cpu\")\n","    print(\"Current device:\", device)\n","else:\n","    device=torch.device(\"cuda\")\n","    print(\"Current device:\", device, \"- Type:\", torch.cuda.get_device_name(0))\n","\n","\n","enc_input_size = 2\n","dec_input_size = 3\n","dec_output_size = 3\n","\n","\n","num_heads = 8\n","d_model = 512\n","dim_feedforward = 2048\n","dropout = 0.1\n","preds_num = 8\n","\n","def subsequent_mask(size):\n","    \"\"\"\n","    Mask out subsequent positions.\n","    \"\"\"\n","    attn_shape = (1, size, size)\n","    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(mask) == 0\n","\n","torch.manual_seed(0)\n","tf = Transformer(enc_input_size, dec_input_size, dec_output_size, N=6,\n","            d_model=d_model, dim_feedforward=dim_feedforward,\n","            num_heads=num_heads, dropout=dropout).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":435,"status":"ok","timestamp":1727281202912,"user":{"displayName":"Leonardo Plini","userId":"13271473654794383474"},"user_tz":-120},"id":"0oHuAm6rA_mG","outputId":"12f12296-f01e-4006-b6e2-c5b646e56144"},"outputs":[],"source":["np.random.seed(0)\n","\n","batch = torch.tensor(np.random.rand(1, 8,4)).float().to(device)\n","inp = batch[:,1:,0:2].to(device)\n","target = batch[:,:-1,2:].to(device)\n","\n","# We create a third mask channel to append to the 2 speeds.\n","# This helps the decoder differentiating between start of sequence token (with mask token 1) and target speeds (with mask token 0)\n","# Summarizing: start_of_seq token is (0,0) and the mask token is 1 ---> [0, 0, 1]\n","#              target inputs are (u_i, v_i) and the mask token is 0 ---> [u_i, v_i, 0]\n","start_of_seq = torch.Tensor([0, 0, 1]).unsqueeze(0).unsqueeze(1).repeat(target.shape[0], 1, 1).to(device)\n","target_c = torch.zeros((target.shape[0], target.shape[1], 1)).to(device)\n","target = torch.cat((target, target_c), -1)\n","# Final decoder input is the concatenation of them along temporal dimension\n","dec_inp = torch.cat((start_of_seq, target), 1)\n","\n","# Source attention is enabled between all the observed input (mask elements are setted to 1)\n","src_att = torch.ones((inp.shape[0], 1, inp.shape[1])).to(device)\n","# For the target attention we mask future elements to prevent model cheating (corresponding future mask elements are setted to False)\n","# The mask is changed dinamically to use teacher forcing learning\n","trg_att = subsequent_mask(dec_inp.shape[1]).repeat(dec_inp.shape[0], 1, 1).to(device)\n","\n","# Source, target and corresponding attention mask are passed to the model for the forward step\n","tf.eval()\n","pred = tf(inp.float(), dec_inp.float(), src_att, trg_att)\n","pred"]},{"cell_type":"markdown","metadata":{"id":"1r81ozQOA_mG"},"source":["### Question (**1 Point**)\n","Q: *Considering the Recurrent Neural Network (RNN) architecture as the previous state-of-the-art for sequence modeling, what are the main advantages of the Transformer architecture?*\n","\n","A: ..."]},{"cell_type":"markdown","metadata":{"id":"6F8ILYzFcht_"},"source":["---\n","## Graph Metanetworks <a class=\"anchor\" id=\"gmn\"></a>\n","\n","This section of the notebook will delve into the paper [Graph Metanetworks for Processing Diverse Neural Architectures](https://arxiv.org/abs/2312.04501v2) by Lim et al., which introduces a novel framework for representing and processing the parameters of neural networks as graph structures. You will find below an explanation of the key paper sections, particularly focusing on the foundational elements from sections 1, 2.1, and 2.3, and you are asked to implement the message passing GNN that processes this graphs.\n","\n","<center width=\"100%\" style=\"padding:25px\"><img src=\"https://drive.google.com/thumbnail?id=1f76OXJFxIr0KZwWAI7nEtSBGEZB9hWN_&sz=w1000\"></center>"]},{"cell_type":"markdown","metadata":{"id":"T0DNCY3HLqbg"},"source":["The paper introduces a new approach about metanetworks (also known as *hypernetworks*), which are neural networks that take the parameters of other neural networks as input. Traditional metanetwork approaches have 2 main limitations:\n","* they are often able to process only specific architectures, such as MLPs or CNNs, hence struggling with generalization;\n","* they consider parameters of neural networks as flattened 1D tensors, hence losing all the structural information of the original network.\n","\n","In \"Graph Metanetworks for Processing Diverse Neural Architectures\" the authors propose a new method called Graph Metanetworks (GMNs), which encodes neural networks as *parameter graphs*. These are graphs where each parameter is represented as an edge between nodes, which in turn represent single neurons or a group of them. It's important to notice that we call these graphs parameter graphs because we design the graphs so that each parameter is associated with a single edge. These graphs are then fed into standard Graph Neural Networks (also known as Message Passing Neural Networks, MPNNs) to be processed while also respecting important structural properties of the original neural network. The GMN approach is conceived to generalize across various architectures such as linear, convolutional and attention layers, normalization layers, ResNet blocks, and so on. The MPNN outputs a single fixed-length representation for each node, edge, and global feature, we can then use what we prefer depending on the downstream task being addressed. For instance, one graph-level task is to predict the scalar accuracy of an input neural network on some dataset. An edge-level task is to predict new weights for an input neural network to change its functionality somehow. Imagination is the limit!\n","\n","### A glimpse on the neural network to parameter graph conversion\n","\n","Every non-recurrent neural network naturally defines a so called \"computation graph\" as a DAG, where nodes are neurons and edges hold neural network parameter weight values. So this already gives us a method to construct a weighted graph, however the computation graph approach has some downsides. First of all, it may be expensive due to weight sharing, and secondly we may want to add more features - such as layer numbers - on top of this graph, to help performance and expressive power when processing it. In the following image there is an example computation graph for a network with a single convolutional layer. The layer has a 2 x 2 filter kernel, a single input and output channel, and applies the filter with a stride of 2. Even in this small case of a 4x4 input image, the computation graph has 16 edges for only 4 parameters due to weight sharing (for visual clarity, bias terms are ignored here):\n","\n","<center width=\"100%\" style=\"padding:25px\"><img src=\"https://drive.google.com/thumbnail?id=1-iFerr4suNhQ_Qza5IpDDCd8JJY__Qja&sz=w500\"></center>\n","\n","Instead, the authors of the paper propose the \"parameter graph\" representation, in which each parameter of the original network is associated with a single edge of the graph (whereas a parameter may be associated to many edges in a computation graph). For the convolutional layer case, the parameter graph construction allocates one node for each input and output channel, we then have parallel edges between each input and output node for each spatial location in the filter kernel, making this a multigraph. One bias node is added, and the bias parameters are encoded as edges from that bias node to each output channel of the layer. The following figure depicts the parameter graph for a convolution layer with a 2 x 2 filter, one input channel, and two output channels. Note that we have two output channels here, unlike the computation graph in the previous figure, where there is only one.\n","\n","<center width=\"100%\" style=\"padding:25px\"><img src=\"https://drive.google.com/thumbnail?id=1odruV6T_bL6vyddbcopJLpi0QRJIDXWd&sz=w500\"></center>\n","\n","The generated parameter graph will have: a feature vector $\\mathbf{v_i}$ associated with each node, a feature vector $\\mathbf{e_{(i,j)}}$ associated with each edge, and a global feature vector $\\mathbf{u}$ associated with the entire graph. The $i^{th}$ node feature vector represents $i^{th}$ node position inside the neural network and its type (linear, conv, normalization, ...). The $\\mathbf{e_{(i,j)}}$ edge feature vector represents the parameter, type and position inside the network of the edge starting from node $j$ and reaching node $i$. The global feature could represent anything from a graph-level label to an embedding of something that we want to inject inside the network."]},{"cell_type":"markdown","metadata":{"id":"ikrOGb_iLqbg"},"source":["### Loading some parameter graphs\n","\n","Here we load 2 pretrained MLPs and build a batch containing 2 parameter graphs, just to gain familiarity with the data representation being used and to test the subsequent code. We will use the PyTorch Geometric library which lets us compose Graph Neural Networks in an easy and modular way, exactly like Pytorch does for standard neural networks. Feel free to look at the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/).\n","\n","The functions ```sequential_to_arch``` and ```arch_to_graph``` are actually converting the neural network into the parameter graph."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6zl9IMZXLqbh","outputId":"9f87c9e4-b345-45f6-f1dd-cedb6f1233bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data(x=[893, 3], edge_index=[2, 52650], edge_attr=[52650, 6], u=[1, 8])\n","Data(x=[893, 3], edge_index=[2, 52650], edge_attr=[52650, 6], u=[1, 8])\n","DataBatch(x=[1786, 3], edge_index=[2, 105300], edge_attr=[105300, 6], u=[2, 8], batch=[1786], ptr=[3])\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\radul\\AppData\\Local\\Temp\\ipykernel_24712\\1650268052.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(\"data/mlp1.pt\", map_location=torch.device(\"cpu\"))['pdata'][-1]\n","C:\\Users\\radul\\AppData\\Local\\Temp\\ipykernel_24712\\1650268052.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(\"data/mlp2.pt\", map_location=torch.device(\"cpu\"))['pdata'][-1]\n"]}],"source":["# create a batch of 2 graphs using torch_geometrics Data and Batch classes\n","import torch\n","from torch_geometric.data import Data, Batch\n","from gmn.graph_construct.model_arch_graph import sequential_to_arch, arch_to_graph\n","\n","torch.manual_seed(0)\n","\n","\n","data = torch.load(\"data/mlp1.pt\", map_location=torch.device(\"cpu\"))['pdata'][-1]\n","arch = sequential_to_arch(data)\n","x, edge_index, edge_attr = arch_to_graph(arch)\n","g_data1 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, u=torch.randn(1, 8))\n","\n","data = torch.load(\"data/mlp2.pt\", map_location=torch.device(\"cpu\"))['pdata'][-1]\n","arch = sequential_to_arch(data)\n","x, edge_index, edge_attr = arch_to_graph(arch)\n","g_data2 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, u=torch.randn(1, 8))\n","\n","batch = Batch.from_data_list([g_data1, g_data2])\n","\n","\n","print(g_data1)\n","print(g_data2)\n","print(batch) # batch.batch has shape [n_nodes], it contains the index of the graph in the batch that each node belongs to"]},{"cell_type":"markdown","metadata":{"id":"LJnv_NlYLqbh"},"source":["In PyTorch Geometric, graph data is typically represented using the ```torch_geometric.data.Data``` class. Each graph is stored as a set of tensors that describe its nodes, edges, and potentially global properties:\n","* **x** contains the node features. Here, a graph has 893 nodes, and each node has 3 features;\n","* **edge_index** encodes the connections (directed edges) between nodes. The shape [2, 52650] means there are 52650 edges in the graph, and the 2 represents the source and target nodes for each edge. The first row of edge_index contains the source nodes of the edges, and the second row contains the target nodes;\n","* **edge_attr** represents the features of the edges. Each of the 52650 edges has 6 features. Note that edge_attr[i] is the feature vector associated with the edge in edge_index[:, i];\n","* **u** represents the global feature vector associated with the graph. The shape [1, 8] indicates there is 1 graph and it has a global feature vector of size 8.\n","\n","Batching is done with the ```torch_geometric.data.Batch``` class, by concatenating the node and edge information of all graphs into a single tensor, as simple as that. In this example, we have a batch of two graphs. You can ignore the **ptr** attribute, while the new **batch** attribute is very useful to keep track of which nodes belong to which graph in the batch. It is a tensor containing for each node the index of the graph in the batch that the node belongs to. For example, the first 893 nodes here would have the value 0 in batch.batch (indicating they belong to the first graph), and the next 893 nodes would have the value 1 (indicating they belong to the second graph).\n","\n","### MPNN implementation\n","\n","We will now implement the Message Passing Neural Network that acts as the backbone in GMN. We will proceed by implementing the single components that process edge features, node features and global features separately, and then merge them together in the final class. Please refer to section 2.3 of the [paper](https://arxiv.org/abs/2312.04501) for more details.\n","\n","The network implemented here is a generalization of a GNN that updates node, edge, and global features all together. For a graph, let $v_i \\in \\mathbb{R}^{d_v}$ be the feature of node $i$, $e_{(i,j)} \\in \\mathbb{R}^{d_e}$ a feature of the directed edge $(i,j)$, $u \\in \\mathbb{R}^{d_u}$ be a global feature associated to the entire graph, and let $E$ be the set of edges in the graph. The directed edge $(i,j)$ represents an edge starting from $j$ and ending at $i$. We allow multigraphs, where there can be several edges (and hence several edge features) between a pair of nodes $(i,j)$; thus, we let $E_{(i,j)}$ denote the set of edge features associated with $(i,j)$."]},{"cell_type":"markdown","metadata":{"id":"t6y81kXifcW2"},"source":["### Edge Model (1 point)\n","\n","In the edge model each edge updates its features based on the features of the nodes it connects and the global feature. The mathematical formulation of the edge model is:\n","\n","$$e_{(i,j)} \\leftarrow \\text{MLP}^{e}(v_i, v_j, e_{(i,j)}, u)$$\n","\n","**IMPORTANT: YOU ARE NOT ALLOWED TO USE FOR LOOPS!**"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"rOP8_lU7fZbC"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import MetaLayer\n","from torch_scatter import scatter\n","\n","class EdgeModel(nn.Module):\n","    def __init__(self, in_dim, out_dim, activation=True):\n","        super().__init__()\n","        if activation:\n","            self.edge_mlp = nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU())\n","        else:\n","            self.edge_mlp = nn.Sequential(nn.Linear(in_dim, out_dim))\n","\n","    def forward(self, src, dest, edge_attr, u, batch):\n","        # **IMPORTANT: YOU ARE NOT ALLOWED TO USE FOR LOOPS!**\n","        # src, dest: [E, F_x], where E is the number of edges. src is the source node features and dest is the destination node features of each edge.\n","        # edge_attr: [E, F_e]\n","        # u: [B, F_u], where B is the number of graphs.\n","        # batch: only here it will have shape [E] with max entry B - 1, because here it indicates the graph index for each edge.\n","\n","        '''\n","        Add your code below\n","        '''\n","        u_batch = u[batch]\n","        edge_input = torch.cat([src, dest, edge_attr, u_batch], dim=1)\n","        updated_edge = self.edge_mlp(edge_input)\n","\n","        return updated_edge"]},{"cell_type":"markdown","metadata":{"id":"1o4ooe6qfjaD"},"source":["### Node Model (2 points)\n","\n","In the node model each node updates its features based on its current feature, the aggregated information from neighboring nodes and edges plus the global feature, and the global feature once again. This is defined by:\n","\n","$$v_i \\leftarrow \\text{MLP}_v^2 \\left(v_i, \\sum_{j, e_{(i,j)} \\in E(i,j)} \\text{MLP}_v^1 (v_i, v_j, e_{(i,j)}, u), u \\right)$$\n","\n","$\\text{MLP}_v^1$ and $\\text{MLP}_v^2$ are Multi-Layer Perceptrons applied to node, edge and global features. The sum aggregates messages from all neighboring nodes $v_j$ connected by edges $e_{(i,j)}$. For the aggregation, please consider using the [scatter](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter) function from pytorch_scatter, the documentation is really clear.\n","\n","**IMPORTANT: YOU ARE NOT ALLOWED TO USE FOR LOOPS!**"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"1HtKmpnMfis-"},"outputs":[],"source":["class NodeModel(nn.Module):\n","    def __init__(self, in_dim_mlp1, in_dim_mlp2, out_dim, activation=True, reduce='sum'):\n","        super().__init__()\n","        self.reduce = reduce\n","        if activation:\n","            self.node_mlp_1 = nn.Sequential(nn.Linear(in_dim_mlp1, out_dim), nn.ReLU())\n","            self.node_mlp_2 = nn.Sequential(nn.Linear(in_dim_mlp2, out_dim), nn.ReLU())\n","        else:\n","            self.node_mlp_1 = nn.Sequential(nn.Linear(in_dim_mlp1, out_dim))\n","            self.node_mlp_2 = nn.Sequential(nn.Linear(in_dim_mlp2, out_dim))\n","\n","    def forward(self, x, edge_index, edge_attr, u, batch):\n","        # **IMPORTANT: YOU ARE NOT ALLOWED TO USE FOR LOOPS!**\n","        # x: [N, F_x], where N is the number of nodes.\n","        # edge_index: [2, E] with max entry N - 1.\n","        # edge_attr: [E, F_e]\n","        # u: [B, F_u]\n","        # batch: [N] with max entry B - 1.\n","\n","        '''\n","        Add your code below\n","        '''\n","        u_batch = u[batch]\n","        ### MLP1 section ###\n","        dest, src = edge_index\n","        edge_info = torch.cat([x[src], edge_attr], dim=1)\n","        \n","\n","        edge_messages = self.node_mlp_1(edge_info)\n","\n","\n","        # aggregation of all messages\n","        aggregated_messages = scatter(edge_messages, dest, dim=0, reduce=self.reduce)\n","\n","        ### MLP2 section ###\n","        node_info = torch.cat([x, aggregated_messages, u_batch], dim=1)\n","\n","        updated_node_features = self.node_mlp_2(node_info)\n","\n","        return updated_node_features"]},{"cell_type":"markdown","metadata":{"id":"Mk8vaFPqfoR1"},"source":["### Global Model (1 point)\n","\n","The global feature $u$ is updated based on aggregations of all node and edge features, and the global feature itself:\n","\n","$$u \\leftarrow \\text{MLP}_u \\left( \\sum_i v_i, \\sum_{e \\in E} e, u \\right)$$\n","\n","**IMPORTANT: YOU ARE NOT ALLOWED TO USE FOR LOOPS!**"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"kHC2JtBmfqPP"},"outputs":[],"source":["class GlobalModel(nn.Module):\n","    def __init__(self, in_dim, out_dim, activation=True, reduce='sum'):\n","        super().__init__()\n","        if activation:\n","            self.global_mlp = nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU())\n","        else:\n","            self.global_mlp = nn.Sequential(nn.Linear(in_dim, out_dim))\n","        self.reduce = reduce\n","\n","    def forward(self, x, edge_index, edge_attr, u, batch):\n","        #**IMPORTANT: YOU ARE NOT ALLOWED TO USE FOR LOOPS!**\n","        # x: [N, F_x], where N is the number of nodes.\n","        # edge_index: [2, E] with max entry N - 1.\n","        # edge_attr: [E, F_e]\n","        # u: [B, F_u]\n","        # batch: [N] with max entry B - 1.\n","\n","        '''\n","        Add your code below\n","        '''\n","        # Aggregate node features\n","        print(\"SONO NEL GLOBAL MODEL\")\n","        u_batch = u[batch]\n","        node_aggregated = scatter(x, batch, dim=0, reduce=self.reduce)\n","        \n","        # Aggregate edges features\n","        edge_batch = batch[edge_index[0]]\n","        edge_aggregated = scatter(edge_attr, edge_batch, dim=0, reduce=self.reduce)\n","        \n","        # Concatenare le informazioni dei nodi aggregati, degli archi aggregati e del vettore globale u\n","        global_info = torch.cat([node_aggregated, edge_aggregated, u_batch], dim=1)\n","        \n","        # Pass to MLP\n","        updated_global = self.global_mlp(global_info)\n","        \n","        return updated_global"]},{"cell_type":"markdown","metadata":{"id":"WMUKNMm4fsQp"},"source":["### MPNN (2 points)\n","\n","Now you are asked to put pieces together and build the complete MPNN. We will use the [torch_geometric.nn.MetaLayer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaLayer.html) class, which helps us by handling some of the scaffold code. Please notice that inside the MetaLayer firstly the edge model is applied, then the node model and then the global one, each of them using the output of the previous model. If you need, give a look to the [source code](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/meta.html#MetaLayer) of the class to better understand the shapes of the tensors that you have to expect."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"hrkAbFMtfunc"},"outputs":[],"source":["class MPNN(nn.Module):\n","\n","    def __init__(self, node_in_dim, edge_in_dim, global_in_dim, hidden_dim, node_out_dim, edge_out_dim, global_out_dim, num_layers,\n","                use_bn=True, dropout=0.0, reduce='sum'):\n","        super().__init__()\n","        self.convs = nn.ModuleList()\n","        self.node_norms = nn.ModuleList()\n","        self.edge_norms = nn.ModuleList()\n","        self.global_norms = nn.ModuleList()\n","        self.use_bn = use_bn\n","        self.dropout = dropout\n","        self.reduce = reduce\n","\n","        assert num_layers >= 2\n","\n","        '''\n","        Instantiate the first layer models with correct parameters below\n","        '''\n","\n","        edge_model = EdgeModel(in_dim=edge_in_dim + 2 * node_in_dim + global_in_dim, out_dim=hidden_dim)\n","        node_model = NodeModel(in_dim_mlp1=node_in_dim + hidden_dim, in_dim_mlp2=node_in_dim + hidden_dim + global_in_dim, out_dim=hidden_dim)\n","        global_model = GlobalModel(in_dim=node_in_dim + edge_in_dim + global_in_dim, out_dim=hidden_dim)\n","        self.convs.append(MetaLayer(edge_model=edge_model, node_model=node_model, global_model=global_model))\n","        self.node_norms.append(nn.BatchNorm1d(hidden_dim))\n","        self.edge_norms.append(nn.BatchNorm1d(hidden_dim))\n","        self.global_norms.append(nn.BatchNorm1d(hidden_dim))\n","\n","        for _ in range(num_layers-2):\n","            '''\n","            Add your code below\n","            '''\n","            # add batch norm after each MetaLayer\n","            edge_model = EdgeModel(in_dim=edge_in_dim + 2 * node_in_dim + global_in_dim, out_dim=hidden_dim)\n","            node_model = NodeModel(in_dim_mlp1=node_in_dim + hidden_dim, in_dim_mlp2=node_in_dim + hidden_dim + global_in_dim, out_dim=hidden_dim)\n","            global_model = GlobalModel(in_dim=node_in_dim + edge_in_dim + global_in_dim, out_dim=hidden_dim)\n","            self.convs.append(MetaLayer(edge_model=edge_model, node_model=node_model, global_model=global_model))\n","            self.node_norms.append(nn.BatchNorm1d(hidden_dim))\n","            self.edge_norms.append(nn.BatchNorm1d(hidden_dim))\n","            self.global_norms.append(nn.BatchNorm1d(hidden_dim))\n","\n","\n","        '''\n","        Add your code below\n","        '''\n","        # last MetaLayer without batch norm and without using activation functions\n","        edge_model = EdgeModel(in_dim=edge_in_dim + 2 * node_in_dim + global_in_dim, out_dim=hidden_dim, activation=False)\n","        node_model = NodeModel(in_dim_mlp1=node_in_dim + hidden_dim, in_dim_mlp2=node_in_dim + hidden_dim + global_in_dim, out_dim=hidden_dim, activation=False)\n","        global_model = GlobalModel(in_dim=node_in_dim + edge_in_dim + global_in_dim, out_dim=hidden_dim, activation=False)\n","        self.convs.append(MetaLayer(edge_model=edge_model, node_model=node_model, global_model=global_model))\n","\n","\n","    def forward(self, x, edge_index, edge_attr, u, batch, *args):\n","\n","        for i, conv in enumerate(self.convs):\n","            '''\n","            Add your code below\n","            '''\n","            x, edge_attr, u = conv(x, edge_index, edge_attr, u, batch)\n","\n","            if i != len(self.convs)-1 and self.use_bn:\n","                '''\n","                Add your code below this line, but before the dropout\n","                '''\n","                x = self.node_norms[i](x)\n","                edge_attr = self.edge_norms[i](edge_attr)\n","                u = self.global_norms[i](u)\n","\n","\n","\n","            x = F.dropout(x, p=self.dropout, training=self.training)\n","            edge_attr = F.dropout(edge_attr, p=self.dropout, training=self.training)\n","            u = F.dropout(u, p=self.dropout, training=self.training)\n","\n","        return x, edge_attr, u\n"]},{"cell_type":"markdown","metadata":{"id":"fLdEPRIvLqbi"},"source":["**Do not change the following code. It is used as a sanity check to verify the good implementation of your code.**"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"HUDqe9K0Lqbi","outputId":"11e90f24-9a44-4069-bbab-12a877bc62bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["src shape: torch.Size([105300, 3])\n","dest shape: torch.Size([105300, 3])\n","edge_attr shape: torch.Size([105300, 6])\n","u shape: torch.Size([2, 8])\n","u_batch shape: torch.Size([105300, 8])\n","edge_input shape: torch.Size([105300, 20])\n","updated_edge shape: torch.Size([105300, 16])\n","Edge_info shape: torch.Size([105300, 19])\n","DOPO IL PRIMO MLP\n"]},{"ename":"NameError","evalue":"name 'scatter' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[32], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m MPNN(node_in_dim, edge_in_dim, global_in_dim, hidden_dim, node_out_dim, edge_out_dim, global_out_dim, num_layers, use_bn, dropout, reduce)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# use the batch object created before\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m x, edge_attr, u \u001b[38;5;241m=\u001b[39m model(batch\u001b[38;5;241m.\u001b[39mx, batch\u001b[38;5;241m.\u001b[39medge_index, batch\u001b[38;5;241m.\u001b[39medge_attr, batch\u001b[38;5;241m.\u001b[39mu, batch\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, edge_attr\u001b[38;5;241m.\u001b[39mshape, u\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n","File \u001b[1;32mc:\\Users\\radul\\anaconda3\\envs\\AML_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\radul\\anaconda3\\envs\\AML_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","Cell \u001b[1;32mIn[31], line 58\u001b[0m, in \u001b[0;36mMPNN.forward\u001b[1;34m(self, x, edge_index, edge_attr, u, batch, *args)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Add your code below\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     x, edge_attr, u \u001b[38;5;241m=\u001b[39m conv(x, edge_index, edge_attr, u, batch)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bn:\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m        Add your code below this line, but before the dropout\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m        '''\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\radul\\anaconda3\\envs\\AML_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\radul\\anaconda3\\envs\\AML_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[1;32mc:\\Users\\radul\\anaconda3\\envs\\AML_env\\Lib\\site-packages\\torch_geometric\\nn\\models\\meta.py:147\u001b[0m, in \u001b[0;36mMetaLayer.forward\u001b[1;34m(self, x, edge_index, edge_attr, u, batch)\u001b[0m\n\u001b[0;32m    143\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_model(x[row], x[col], edge_attr, u,\n\u001b[0;32m    144\u001b[0m                                 batch \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m batch[row])\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_model(x, edge_index, edge_attr, u, batch)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_model(x, edge_index, edge_attr, u, batch)\n","File \u001b[1;32mc:\\Users\\radul\\anaconda3\\envs\\AML_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\radul\\anaconda3\\envs\\AML_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","Cell \u001b[1;32mIn[30], line 35\u001b[0m, in \u001b[0;36mNodeModel.forward\u001b[1;34m(self, x, edge_index, edge_attr, u, batch)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOPO IL PRIMO MLP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# aggregation of all messages\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m aggregated_messages \u001b[38;5;241m=\u001b[39m scatter(edge_messages, dest, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m### MLP2 section ###\u001b[39;00m\n\u001b[0;32m     38\u001b[0m node_info \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, aggregated_messages, u_batch], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'scatter' is not defined"]}],"source":["torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","np.random.seed(0)\n","random.seed(0)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# try the MPNN model\n","node_in_dim = 3\n","edge_in_dim = 6\n","global_in_dim = 8\n","hidden_dim = 16\n","node_out_dim = 8\n","edge_out_dim = 8\n","global_out_dim = 8\n","num_layers = 3\n","use_bn = True\n","dropout = 0.1\n","reduce = 'sum'\n","\n","model = MPNN(node_in_dim, edge_in_dim, global_in_dim, hidden_dim, node_out_dim, edge_out_dim, global_out_dim, num_layers, use_bn, dropout, reduce)\n","\n","# use the batch object created before\n","x, edge_attr, u = model(batch.x, batch.edge_index, batch.edge_attr, batch.u, batch.batch)\n","print(x.shape, edge_attr.shape, u.shape)\n","print()\n","print(x)\n","print()\n","print(edge_attr)\n","print()\n","print(u)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["le48bnfJA_lt"],"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Ambiente AML","language":"python","name":"aml_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}
